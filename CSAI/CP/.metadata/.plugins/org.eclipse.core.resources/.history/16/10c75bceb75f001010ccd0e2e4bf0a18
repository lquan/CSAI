import weka.core.converters.ConverterUtils.DataSource;
import weka.core.converters.ArffSaver;
import weka.core.Instance;
import weka.core.Instances;
import java.io.*;
import java.util.*;

class SetMiner {
	
	
	private static final boolean PRUNE = true;

	private static final double RULECOST = 5;

	private static final int MAX_NB_LEAF = 3;
	
	private static final boolean PREDICT_AVERAGE = true;
	
	
	/**
 	* een eerste arff bestandsnaam, verwijzend naar het bestand met trainingsvoorbeelden (invoer)
    * 
    * een tweede arff bestandsnaam, verwijzend naar het bestand met testvoorbeelden; hierop moet je algoritme voorspellingen gaan doen (invoer)
    * 
    * een derde arff bestandsnaam, verwijzend naar het bestand waarin voor elk testvoorbeeld in de laatste kolom opgeslagen wordt wat de voorspelling is 
    * (voorbeelden in deze arff moeten dezelfde volgorde hebben als in de arff met testvoorbeelden) (uitvoer)
    *
    * een vierde arff bestandsnaam, waarin de gevonden itemsets worden opgeslagen. 
   	* In dit bestand moet elke rij één gevonden itemset bevatten. 
    * De itemset moet voorgesteld worden door een 1 te plaatsen in de kolommen overeenkomstig met de gevonden items. (uitvoer)
	 */
	
	private static Instances normalize(Instances in, double factor) {
		Instances out = new Instances(in);
		
		Instance instance;
		for (int i=0; i < out.numInstances(); ++i) {
			instance = out.instance(i);
			instance.setClassValue(instance.classValue()*1.0/factor);		
		}
		return out;
		
		
	}
	
	private static double findMax(Instances in) {
		double[] values = new double[in.numInstances()];
		
		for (int i =0; i < in.numInstances(); ++i) {
			values[i] = in.instance(i).classValue();
		}
		
		Arrays.sort(values);
		
		return values[values.length-1];
		
		
	}

	public static void main(String[] args) {
		if (args.length < 4) {
			throw new IllegalArgumentException("usage: 4 parameters!!!");
		}
		
		try {
			// Read the data specified in the command line
			DataSource source = new DataSource(args[0]);
			Instances data = source.getDataSet();
			if (data.classIndex() == -1)
				data.setClassIndex(data.numAttributes() - 1);
			
			double scale = findMax(data);
			data = normalize(data, scale, false);
			
			
			Instances predictionsoriginal = new Instances(data); 	// copy the dataset to make predictions for
			Instances itemsets = new Instances(data,0); 			// only headers for itemsets (args[3])
			
			DataSource source2 = new DataSource(args[1]);
			Instances predictions = source2.getDataSet();           //the predictions of args[1] into args[2]
			if (predictions.classIndex() == -1)
				predictions.setClassIndex(predictions.numAttributes() - 1);
			
			
			// The recursive iterative rule searching
			splitData(data, null, predictionsoriginal, itemsets, new int[0]);
			data = normalize(data,scale, true);
			predictionsoriginal = normalize(data,scale,true);
			
			
			
			//stat.tot, stat.prediction, stat.value
			
			
			// Prune the rule set
			if (PRUNE) {
				itemsets = pruneRules(data, predictionsoriginal, itemsets);
			}
			
			double error = calcError(data, predictionsoriginal);
			System.out.println("the error on the training set is: " + error);
			System.out.println("number of rules " + itemsets.numInstances());
			
			applyRules(itemsets, predictionsoriginal);
			error = calcError(data, predictionsoriginal);
			System.out.println("the error on the training set is: " + error);
			System.out.println("number of rules " + itemsets.numInstances());
			
			// do the predictions for the given instances
			applyRules(itemsets, predictions);
			/*System.out.println(predictions);
			System.out.println();
			System.out.println( data);*/
			
			// Write the data to a arff file: the two output arff files 
			writeData(predictions, args[2]);
			writeData(itemsets, args[3]);
			// + 1 for the original training examples
			writeData(predictionsoriginal, "predictionsoriginal.arff");

//			ArffSaver saver = new ArffSaver();
//			
//			saver.setInstances(predictions);
//			saver.setFile(new File(args[2]));
//			saver.writeBatch();
//			
//			saver.setInstances(itemsets);
//			saver.setFile(new File(args[3])); // args[3]
//			saver.writeBatch();
//			
//			// and also the predictions on the original dataset for comparison
//			saver.setInstances(predictionsoriginal);
//			saver.setFile(new File("predictionsoriginal.arff"));
//			saver.writeBatch();
		}
		catch ( Exception e ) {
			e.printStackTrace();
		}
	}



	private static void applyRules(Instances itemsets, Instances predictions) {
		for (int i=0; i<predictions.numInstances(); ++i) {
			Instance instance = predictions.instance(i);
			boolean covered = true;
			for (int j= 0; j<itemsets.numInstances(); ++j) {
				Instance rule = itemsets.instance(j);
				covered = true;
				for (int k=0; k< itemsets.numAttributes() -1 ; ++k) {
					if (rule.value(k)==1) {
						covered = instance.value(k)==1;
						if(!covered){
							break;
						}
					}
					//else covered = true;
					
				}
				if (covered) {
					
					instance.setClassValue(rule.classValue());
					break;
				}
			}
		}
	}



	private static double calcError(Instances data, Instances data2) {
		// calculate the total l1 norm of the class values of the two data sets 
		double error = 0;
		for (int i=0; i < data.numInstances(); ++i) {
			error += Math.abs(data.instance(i).classValue() - data2.instance(i).classValue());			
		}
		return error;
	}

	
	
	
	
	
	
	private static void writeData(Instances data, String filename) {
		try {
			ArffSaver saver = new ArffSaver();
			saver.setInstances(data);
			saver.setFile(new File(filename));
			saver.writeBatch();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}
	
	/**
	 * 
	 * @param data		original training data (with original labels)
	 * @param predOr	training data with predicted labels
	 * @param itemsets	item sets to be used as rules
	 */
	private static Instances pruneRules(Instances data, Instances predOr, Instances itemsets){
		int originalSize;
		int newSize;
		Instances newPredOr = new Instances(predOr);
//		Instances itemsets2 = new Instances(itemsets);
		do{
			originalSize = itemsets.numInstances();
			itemsets = pruneRule(data, newPredOr, itemsets);
			newSize = itemsets.numInstances();
			applyRules(itemsets,newPredOr);
//			itemsets = itemsets2;
		}
		while(newSize < originalSize);
		return itemsets;
	}
	
	private static Instances pruneRule(Instances data, Instances predOr, Instances itemsets) {
		Instances bestCopy = itemsets;
		Instances workingData = new Instances(predOr);
		double bestError = calcError(data, predOr);
//		System.out.println("try prune rule, error so far: " +bestError);
		for(int i = itemsets.numInstances() - 2; i > 0 ; i--){
			Instances itemsetCopy = new Instances(itemsets);
//			System.out.println("rule removed : "+ itemsetCopy.instance(i));
			itemsetCopy.delete(i);
//			System.out.println(data);
			applyRules(itemsetCopy, workingData);
//			System.out.println(data);
			double error = calcError(data, workingData);
//			System.out.println("error for if "+error);
			
			if( calcNewError(error) < bestError ){ 
				//System.out.println("bla");
				bestError = calcNewError(error);
				bestCopy = itemsetCopy;
			}
			//System.out.println("best error" + bestError);
		}
		return bestCopy;
	}
	
	private static double calcNewError(double error) {
		return error - RULECOST;
	}
	

	
	private static void splitData(Instances data, List<Integer> indices, Instances result, Instances itemsets, int[] items) throws Exception {
		if (data.numInstances() <= MAX_NB_LEAF) {
			double prediction =  getPrediction(data,PREDICT_AVERAGE); //for average set true
			// System.out.println(prediction);
			// update the class labels of the selected
			for (int i : indices) {
				result.instance(i).setClassValue(prediction);
			}
			
			//System.out.println(result);
			double[] attributes = new double[data.numAttributes()];
			for (int i : items) {
				attributes[i] = 1;
			}
			attributes[attributes.length-1] = prediction;
			itemsets.add(new Instance(1, attributes ));
		}
		else{
			int[] items2 = findPattern(data);
			InstanceSelection data2 = select(data, items2);
			items2 = union(items2, items);
			
			double[] attributes = new double[data.numAttributes()];
			for (int i : items) {
				attributes[i] = 1;
			}			
			
			if (data2.getSelected().numInstances() != 0) {
				//Instances oldItems = new Instances(itemsets);
				splitData(data2.getSelected(), data2.getIndicesSelected(), result, itemsets, items2);			
			}
			
			if (data2.getNotSelected().numInstances() != 0) {
				splitData(data2.getNotSelected(), data2.getIndicesNotSelected(), result, itemsets, items);
			}
			
			
		}
		
	}


	private static int[] union(int[] items1, int[] items2) {
		Set<Integer> itemSet = new HashSet<Integer>();
		for(int i : items1) {
			itemSet.add(i);
		}
		for(int i : items2){
			itemSet.add(i);
		}
		int[] items = new int[itemSet.size()];
		int i = 0;
		for(int val : itemSet){
			items[i++] = val; 
		}
		return items;
	}

	//================================================== median
	//  Precondition: Array must be sorted
	private static double median(double[] m) {
		int middle = m.length/2;  // subscript of middle element
		if (m.length%2 == 1) {
			// Odd number of elements -- return the middle one.
			return m[middle];
		} else {
			// Even number -- return average of middle two
			// Must cast the numbers to double before dividing.
			return (m[middle-1] + m[middle]) / 2.0;
		}
	}//end method median


	private static double getPrediction(Instances selected, boolean average) {
		if (average)
			return selected.meanOrMode(selected.numAttributes() - 1);
		else {
			double[] values = selected.attributeToDoubleArray(selected.numAttributes() - 1);
			Arrays.sort(values);
			return median(values);
		}
	}

	/***
	 * Apply the constraint programming system to find the 'best' pattern in a dataset
	 * @param data
	 * @return int[] best pattern in the given dataset
	 * @throws Exception
	 */
	public static int[] findPattern ( Instances data ) throws Exception {
		// Write the data to a arff file
		ArffSaver saver = new ArffSaver();
		saver.setInstances(data);
		saver.setFile(new File("tmpdata.arff"));
		saver.writeBatch();

		// Run the CP system on the dataset - update the command line to the appropriate path
		Runtime rt = Runtime.getRuntime();

		// command AND environment path
		String[] command = {"rimcp/src/rimcp", "-datafile", "tmpdata.arff", "-solfile", "tmppattern"};
		String[] envpath = {"LD_LIBRARY_PATH=rimcp/lib/gecode-3.4.2/"};

		Process pr = rt.exec(command,envpath);
		System.out.println("Running rimcp on " + data.numInstances () + " instances" );

		int exitVal = pr.waitFor();
		System.out.println("Exited with error code "+exitVal);

		// Read the pattern file written by the CP system
		BufferedReader in = new BufferedReader(new FileReader("tmppattern"));
		String line, lastline = "";
		if (!in.ready())
			throw new IOException();
		while ((line = in.readLine()) != null) {
			lastline = line;
		}
		System.out.println("Pattern found: " + lastline );

		// Parse the last line (containing the 'best' pattern) into an array of items
		String items[] = lastline.split(" ");
		int items2 [] = new int[items.length-2];

		for ( int i = 0; i <items.length-2; i++ ) {
			items2[i] = Integer.parseInt(items[i]);
			System.out.println(data.attribute ( items2[i] ).name ());
		}
		in.close();

		System.out.println();
		// Return the set of items
		return items2;
	}
	
	/**
	 * Selects instances according to the itemset
	 * @param data 
	 * @param items
	 * @return instanceselection
	 */
	public static InstanceSelection select ( Instances data, int items[] ) {
		/*Instances data2 = new Instances ( data, 0 );
		Instances data3 = new Instances(data,0);
		Instances[] result = {data3, data2};*/
		InstanceSelection result = new InstanceSelection(data);
		
		boolean isCovered = false;
		
		for ( int i = 0; i < data.numInstances (); ++i ) {
			isCovered = false;
			for ( int j = 0; j < items.length; ++j ) {
				if ( data.instance ( i ).value ( items[j] ) == 0 ) {
					isCovered = false;
					// one item is missing, add immediately
					//result.add ( data.instance ( i ) );
					result.addNotSelected(data.instance(i));
					result.addIndicesNotSelected(i);
					break;
				}
				
				else {
					isCovered = true;
				}
			}
			
			if (isCovered) {
				//data3.add(data.instance(i));
				result.addSelected(data.instance(i));
				result.addIndicesSelected(i);
			}
		}
		
		return result;
	}
	
}
